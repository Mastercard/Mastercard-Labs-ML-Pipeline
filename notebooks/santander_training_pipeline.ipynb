{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Santander Customer Transaction Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image](imgs/sant_comp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem and data\n",
    "- Identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Anonymized dataset containing `numeric feature variables`, the binary `target column`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The task is to predict the value of binary `target` column in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# End-to-end real world ML pipeline\n",
    "![image](imgs/demo_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Kubeflow Pipelines is a platform for building and deploying portable, scalable machine learning (ML) workflows based on docker containers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Kubeflow pipelines make it easy to implement production grade machine learning workflows without worrying about the low-level details of managing a Kubernetes cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![image](imgs/kf_goals1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " - Visual depiction of pipeline topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For any given step you can use libraries and tools of choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A single step cloud be a single node instance GPU or CPU / or something that can run on a distributed fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Easily convert a dockerized task into a pipeline step by following specific format of how the docker container accepts input and produces output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Python SDK to specify the sequence of steps of your workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ML Workflow Orchestration\n",
    "\n",
    "![image](imgs/example_pipeline1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals\n",
    "![image](imgs/kf_goals2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Package the define pipeline up as a zip file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Even an individual step can be packaged up as a reusable component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kubeflow Pipelines Goals\n",
    "![image](imgs/kf_goals3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Rapidly iterate on your ideas in a reliable and manageable way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Historical view of any prior runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Filters to find past runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Clone a past run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Comparison feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### 1. Containerized implementations of ML tasks\n",
    "- Docker containers provide portability, repeatability and encapsulation\n",
    "- Any tools or libraries could be adopted inside your containerized docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?\n",
    "### 2. Specification of the sequence of steps\n",
    "- Specification of the sequence of steps that should run, how the data flows between these steps and how to connect the output of one step with the inputs of downstream step\n",
    "- Model multi-step workflows as a sequence of steps or capture the dependencies between tasks using a graph (DAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What Constitutes a Kubeflow Pipeline?\n",
    "### 3. Input Parameters\n",
    "- Defining input parameters that will be exposed to the end user in a UI form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Santander Customer Transaction Prediction: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "(train_input_fn, feature_names, feature_columns) = \\\n",
    "    make_inputs_from_np_arrays(features_np=train_data[:, 1:],\n",
    "                               label_np=train_data[:, 0:1])\n",
    "classifier = tf.estimator.BoostedTreesClassifier(\n",
    "    feature_columns,\n",
    "    n_batches_per_layer=args.num_batch,\n",
    "    model_dir=args.job_dir,\n",
    "    n_trees=args.n_trees,\n",
    "    max_depth=args.max_depth,\n",
    "    learning_rate=args.learning_rate,\n",
    "    )\n",
    "classifier.train(train_input_fn)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "EXP_NAME='Santander Training Pipeline'\n",
    "OUTPUT_DIR='gs://kubeflow-pipelines-demo/tfx'\n",
    "\n",
    "PROJECT_NAME = 'kf-pipelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kfp in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (0.1.31.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from kfp) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: Deprecated in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (1.2.6)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: PyJWT>=1.6.4 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: argo-models==2.2.1a in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (2.2.1a0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate==0.8.3 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (0.8.3)\n",
      "Requirement already satisfied, skipping upgrade: click==7.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: kfp-server-api<=0.1.25,>=0.1.18 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (0.1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.15 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from kfp) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: kubernetes<=9.0.0,>=8.0.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (8.0.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: cryptography>=2.4.2 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (2.6.1)\n",
      "Requirement already satisfied, skipping upgrade: jsonschema>=3.0.1 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: requests-toolbelt>=0.8.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (0.9.1)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.6.1 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from kfp) (1.6.1)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.13.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kfp) (1.13.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from Deprecated->kfp) (1.10.11)\n",
      "Requirement already satisfied, skipping upgrade: requests-oauthlib in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp) (0.56.0)\n",
      "Requirement already satisfied, skipping upgrade: adal>=1.0.2 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: requests in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=21.0.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp) (40.6.2)\n",
      "Requirement already satisfied, skipping upgrade: cffi!=1.11.3,>=1.8 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from cryptography>=2.4.2->kfp) (1.12.3)\n",
      "Requirement already satisfied, skipping upgrade: asn1crypto>=0.21.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from cryptography>=2.4.2->kfp) (0.24.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (0.23)\n",
      "Requirement already satisfied, skipping upgrade: pyrsistent>=0.14.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (0.15.4)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from jsonschema>=3.0.1->kfp) (19.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-auth>=1.6.1->kfp) (4.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<0.29dev,>=0.28.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.28.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core<2.0.0dev,>=0.1.1 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media>=0.3.1 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-cloud-storage>=1.13.0->kfp) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from requests-oauthlib->kubernetes<=9.0.0,>=8.0.0->kfp) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from requests->kubernetes<=9.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.4.2->kfp) (2.19)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from importlib-metadata->jsonschema>=3.0.1->kfp) (0.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.1 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.4)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos!=1.5.4,<2.0dev,>=1.5.3 in /Users/ahmedmenshawy/.local/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (1.5.5)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (2018.7)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.4.0 in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=0.1.1->google-cloud-storage>=1.13.0->kfp) (3.6.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools in /anaconda2/envs/settlementpred/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata->jsonschema>=3.0.1->kfp) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Create an Experiment in the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kubeflow Pipeline requires having an _experiment_ before making a run. An experiment is a group of comparable runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/27f3a67e-aaa3-426e-b0c6-5e8a1dcd2c4a\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import compiler\n",
    "import kfp.dsl as dsl\n",
    "\n",
    "import kfp.notebook\n",
    "import kfp.gcp as gcp\n",
    "\n",
    "client = kfp.Client()\n",
    "exp = client.create_experiment(name=EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Authoring a pipeline is just like authoring a normal Python function with a sepcial decoration for the SDK to recognize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The pipeline function describes the topology of the pipeline and how the input of one component is passed down the stream as an output to another component. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the below pipeline, all the docker container images referenced in the pipeline are already built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp import gcp\n",
    "from kfp import onprem\n",
    "\n",
    "platform = 'GCP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convert a dockerized task into a pipeline step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Define a python function wrapper for that container by invoking an sdk function ContainerOp\n",
    "\n",
    "```python\n",
    "def dataproc_train_op(\n",
    "    project,\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    output\n",
    "):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Dataproc - Train XGBoost model',\n",
    "        image='gcr.io/ml-pipeline/xgboost-training:v1.1',\n",
    "        arguments=[\n",
    "            '--project', project,\n",
    "            '--train', train_data,\n",
    "            '--eval', eval_data,\n",
    "            '--output', output,\n",
    "        ],\n",
    "        file_outputs={\n",
    "            'output': '/output.txt',\n",
    "        }\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define a docker container specification file\n",
    "```yaml\n",
    "name: Train Boosted Trees\n",
    "description: Trains Boosted Trees using Tensorflow\n",
    "inputs:\n",
    "  - {name: Transformed data dir,  type: GCSPath, description: 'Path to transformed data'}\n",
    "  - {name: Learning rate,type: Float, default: '0.1', description: 'Learning rate for training.'}\n",
    "outputs:\n",
    "  - {name: Training output dir, type: GCSPath, description: 'GCS or local directory.'} \n",
    "implementation:\n",
    "  container:\n",
    "    image: us.gcr.io/kf-pipelines/kubeflow-train_boosted:v0.3\n",
    "    command: [python, -m, trainer.boosted]\n",
    "    args: [\n",
    "      --transformed-data-dir, {inputValue: Transformed data dir},\n",
    "      --learning-rate, {inputValue: Learning rate},\n",
    "      --steps, {inputValue: Steps},\n",
    "    ]\n",
    "    fileOutputs:\n",
    "      Training output dir: /output.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Load components (steps) from specification YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataflow_tf_transform_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/preprocessing/tft/component.yaml')\n",
    "tf_train_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/training/dnntrainer/component.yaml')\n",
    "dataflow_tf_predict_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/training/predict/component.yaml')\n",
    "\n",
    "confusion_matrix_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/metrics/confusion_matrix/component.yaml')\n",
    "roc_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/metrics/roc/component.yaml')\n",
    "\n",
    "kubeflow_deploy_op = \\\n",
    "    components.load_component_from_file('../pipeline_steps/serving/deployer/component.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Define pipeline function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Santander Customer Transaction Prediction',\n",
    "    description='Example pipeline that does classification with model analysis based on Santander customer transaction dataset.'\n",
    ")\n",
    "def santander_transaction_classification(\n",
    "        output,\n",
    "        project,\n",
    "        train='gs://kubeflow-pipelines-demo/dataset/train.csv',\n",
    "        evaluation='gs://kubeflow-pipelines-demo/dataset/test.csv',\n",
    "        mode='local',\n",
    "        preprocess_module='gs://kubeflow-pipelines-demo/dataset/preprocessing.py',\n",
    "        learning_rate=0.1,\n",
    "        hidden_layer_size='1500',\n",
    "        steps=3000\n",
    "):\n",
    "    output_template = str(output) + '/{{workflow.uid}}/{{pod.name}}/data'\n",
    "    target_class_lambda = \"\"\"lambda x: x['target']\"\"\"\n",
    "\n",
    "    tf_server_name = 'kfdemo-service'\n",
    "\n",
    "    if platform != 'GCP':\n",
    "        vop = dsl.VolumeOp(\n",
    "            name=\"create_pvc\",\n",
    "            resource_name=\"pipeline-pvc\",\n",
    "            modes=dsl.VOLUME_MODE_RWM,\n",
    "            size=\"1Gi\"\n",
    "        )\n",
    "\n",
    "        checkout = dsl.ContainerOp(\n",
    "            name=\"checkout\",\n",
    "            image=\"alpine/git:latest\",\n",
    "            command=[\"git\", \"clone\", \"https://github.com/kubeflow/pipelines.git\", str(output) + \"/pipelines\"],\n",
    "        ).apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n",
    "        checkout.after(vop)\n",
    "\n",
    "    preprocess = dataflow_tf_transform_op(\n",
    "        training_data_file_pattern=train,\n",
    "        evaluation_data_file_pattern=evaluation,\n",
    "        schema=\"not.txt\",\n",
    "        gcp_project=project,\n",
    "        run_mode=mode,\n",
    "        preprocessing_module=preprocess_module,\n",
    "        transformed_data_dir=output_template\n",
    "    )\n",
    "\n",
    "    training = tf_train_op(\n",
    "        transformed_data_dir=preprocess.output,\n",
    "        schema='not.txt',\n",
    "        learning_rate=learning_rate,\n",
    "        hidden_layer_size=hidden_layer_size,\n",
    "        steps=steps,\n",
    "        target='tips',\n",
    "        preprocessing_module=preprocess_module,\n",
    "        training_output_dir=output_template\n",
    "    )\n",
    "\n",
    "    prediction = dataflow_tf_predict_op(\n",
    "        data_file_pattern=evaluation,\n",
    "        schema='not.txt',\n",
    "        target_column='tips',\n",
    "        model=training.outputs['training_output_dir'],\n",
    "        run_mode=mode,\n",
    "        gcp_project=project,\n",
    "        predictions_dir=output_template\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix_op(\n",
    "        predictions=prediction.outputs['predictions_dir'],\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    roc = roc_op(\n",
    "        predictions_dir=prediction.outputs['predictions_dir'],\n",
    "        target_lambda=target_class_lambda,\n",
    "        output_dir=output_template\n",
    "    )\n",
    "\n",
    "    steps = [training, prediction, cm, roc]\n",
    "    for step in steps:\n",
    "        if platform == 'GCP':\n",
    "            step.apply(gcp.use_gcp_secret('user-gcp-sa'))\n",
    "        else:\n",
    "            step.apply(onprem.mount_pvc(vop.outputs[\"name\"], 'local-storage', output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compile the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Compile the pipeline into a tar package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(santander_transaction_classification, 'santander_training_pipeline.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submit the run with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/db78b404-f19a-11e9-8c76-42010a8001d2\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "run = client.run_pipeline(exp.id, 'santander_training_pipeline-' + time.strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                          'santander_training_pipeline.zip',\n",
    "                          params={'output': 'gs://kubeflow-pipelines-demo/tfx', 'project': 'kf-pipelines',\n",
    "                                  'train': 'gs://kubeflow-pipelines-demo/dataset/train.csv',\n",
    "                                  'evaluation': 'gs://kubeflow-pipelines-demo/dataset/test.csv', 'mode': 'local',\n",
    "                                  'preprocess_module': 'gs://kubeflow-pipelines-demo/dataset/preprocessing.py',\n",
    "                                  'learning_rate': 0.1, 'hidden_layer_size': '1500', 'steps': 3000})"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
